import cv2
import math
import json
import faiss
import random
import pickle
import numpy as np
import streamlit as st
from typing import Tuple, List
from collections import defaultdict
from deepface.modules import detection, representation

from config import *


# –§—É–Ω–∫—Ü—ñ—ó –¥–ª—è –∫–Ω–æ–ø–æ–∫ –Ω–∞–≤—ñ–≥–∞—Ü—ñ—ó
def decrease_time():
    st.session_state.current_timestamp = max(0.0, st.session_state.current_timestamp - 0.5)
    st.session_state.slider_key += 1  # –ó–º—ñ–Ω—é—î–º–æ –∫–ª—é—á –¥–ª—è –ø–µ—Ä–µ–º–∞–ª—å–æ–≤—É–≤–∞–Ω–Ω—è —Å–ª–∞–π–¥–µ—Ä–∞


def increase_time():
    if 'cap' in st.session_state and st.session_state.cap is not None:
        duration = st.session_state.cap.get(cv2.CAP_PROP_FRAME_COUNT) / st.session_state.cap.get(cv2.CAP_PROP_FPS)
        st.session_state.current_timestamp = min(float(duration), st.session_state.current_timestamp + 0.5)
        st.session_state.slider_key += 1  # –ó–º—ñ–Ω—é—î–º–æ –∫–ª—é—á –¥–ª—è –ø–µ—Ä–µ–º–∞–ª—å–æ–≤—É–≤–∞–Ω–Ω—è —Å–ª–∞–π–¥–µ—Ä–∞


# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –∫–∞–¥—Ä—É –∑–∞ —á–∞—Å–æ–≤–∏–º –∫–æ–¥–æ–º
def get_frame_at_timestamp(cap, timestamp):
    """
    –ü–µ—Ä–µ–º–æ—Ç—É—î–º–æ –≤—ñ–¥–µ–æ –ø–æ –Ω–æ–º–µ—Ä—É –∫–∞–¥—Ä—É –∑–∞–º—ñ—Å—Ç—å POS_MSEC.
    """
    if cap is None or not cap.isOpened():
        return None, False

    fps = cap.get(cv2.CAP_PROP_FPS)
    frame_number = int(timestamp * fps)
    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)

    ret, frame = cap.read()
    if not ret:
        return None, False
    return frame, True

# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –≥–∞—Ä–∞–Ω—Ç—ñ—ó, —â–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –º–∞—î –ø—ñ–¥—Ç—Ä–∏–º—É–≤–∞–Ω–∏–π —Ç–∏–ø –¥–∞–Ω–∏—Ö –¥–ª—è OpenCV
def ensure_uint8(img):
    """Convert image to uint8 if it's not already in that format"""
    if img is None:
        return None

    if isinstance(img, np.ndarray):
        # Check if image is float64 (CV_64F) and convert if needed
        if img.dtype == np.float64:
            # Scale to 0-255 range if needed
            if np.max(img) <= 1.0:
                img = (img * 255).astype(np.uint8)
            else:
                img = img.astype(np.uint8)
        elif img.dtype != np.uint8:
            # Handle other types by converting to uint8
            img = img.astype(np.uint8)

    return img
@st.cache_resource
def load_recognition_resources(
        index_path: str,
        actors_map_path: str,
        thresholds_path: str,
        ukr_actor_names_path: str
):
    # FAISS-—ñ–Ω–¥–µ–∫—Å
    index = faiss.read_index(index_path)

    # –ú–∞–ø–∞ –∞–∫—Ç–æ—Ä—ñ–≤
    with open(actors_map_path, 'rb') as f:
        actor_map = pickle.load(f)

    # –Ü–Ω–¥–∏–≤—ñ–¥—É–∞–ª—å–Ω—ñ —Ç—Ä–µ—à—Ö–æ–ª–¥–∏
    with open(thresholds_path, 'r') as f:
        inference_thresholds = json.load(f)

    # –Ü–º–µ–Ω–∞ –∞–∫—Ç–æ—Ä—ñ–≤ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é
    with open(ukr_actor_names_path, 'r', encoding='utf-8') as f:
        ukr_actor_names = json.load(f)

    return index, actor_map, inference_thresholds, ukr_actor_names


def recognize_actors(
        img: np.ndarray,
        default_threshold: float = 0.4,
        ratio_threshold: float = 1.1,
        model_name: str = 'Facenet512',
        detector_backend: str = 'mtcnn',
        align: bool = True,
        normalization: str = 'base'
) -> List[Tuple[np.ndarray, str, float, dict]]:
    """
    1) –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î FAISS-—ñ–Ω–¥–µ–∫—Å —Ç–∞ actor_map
    2) –î–µ—Ç–µ–∫—Ç—É—î –≤—Å—ñ –æ–±–ª–∏—á—á—è —É img —ñ —Ä–æ–±–∏—Ç—å –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –µ–º–±–µ–¥–∏–Ω–≥
    3) –®—É–∫–∞—î –¥–≤–æ—Ö –Ω–∞–π–±–ª–∏–∂—á–∏—Ö —Å—É—Å—ñ–¥—ñ–≤ (–¥–ª—è ratio-test)
    4) –î–ª—è –∫–æ–∂–Ω–æ–≥–æ –æ–±–ª–∏—á—á—è –ø–æ–≤–µ—Ä—Ç–∞—î –∫–æ—Ä—Ç–µ–∂ (face_img, actor, similarity, facial_area)
       –∞–±–æ (face_img, "Unknown", similarity, facial_area), —è–∫—â–æ –ø–æ—Ä—ñ–≥ –Ω–µ –ø—Ä–æ–π–¥–µ–Ω–∏–π.
    """
    # –ü–µ—Ä–µ–∫–æ–Ω–∞—î–º–æ—Å—å —É –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—ñ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
    img = ensure_uint8(img)

    # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –≤—Å—ñ –ª–∏—Ü—è –Ω–∞ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—ñ
    faces = detection.extract_faces(
        img_path=img,
        detector_backend=detector_backend,
        enforce_detection=False,
        align=align
    )

    h, w = img.shape[:2]

    # –í—ñ–¥—Ñ—ñ–ª—å—Ç—Ä–æ–≤—É—î–º–æ —Ö–∏–±–Ω—ñ –¥–µ—Ç–µ–∫—Ü—ñ—ó –∑–∞ —Ä–æ–∑–º—ñ—Ä–æ–º
    faces = [
        f for f in faces
            if f.get('face') is not None and f['face'].size > 0
            and f['face'].shape[0] > 0 and f['face'].shape[1] > 0
            and f['face'].shape[0] < h and f['face'].shape[1] < w
            ]

    if not faces:
        return None

    results = []

    for face_info in faces:
        # –í–∏—Ç—è–≥—É—î–º–æ —Å–∞–º–µ –ª–∏—Ü–µ –∑ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è —Ç–∞ –π–æ–≥–æ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∏
        face_img = face_info['face']
        facial_area = face_info['facial_area']

        # –ü–µ—Ä–µ–∫–æ–Ω–∞—î–º–æ—Å—å —É –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—ñ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
        face_img = ensure_uint8(face_img)

        # –û—Ç—Ä–∏–º—É—î–º–æ –µ–º–±–µ–¥–∏–Ω–≥
        rep = representation.represent(
            img_path=face_img,
            model_name=model_name,
            enforce_detection=True,
            detector_backend='skip',
            align=align,
            normalization=normalization
        )
        q = np.array(rep[0]['embedding'], dtype=np.float32)
        q /= np.linalg.norm(q)
        q = q.reshape(1, -1)

        # –®—É–∫–∞—î–º–æ 2 –Ω–∞–π–±–ª–∏–∂—á–∏—Ö —Ü–µ–Ω—Ç—Ä–æ—ó–¥–∏
        D, I = INDEX.search(q, 2)
        sim1, sim2 = float(D[0][0]), float(D[0][1])
        actor1 = ACTOR_MAP[int(I[0][0])]

        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ –µ–º–±–µ–¥–∏–Ω–≥ –∑–Ω–∞—Ö–æ–¥–∏—Ç—å—Å—è –±–ª–∏–∑—å–∫–æ –¥–æ –∫—ñ–ª—å–∫–æ—Ö —Ü–µ–Ω—Ç—Ä–æ—ó–¥—ñ–≤ –æ–¥—Ä–∞–∑—É
        if sim1 / (sim2 + 1e-8) < ratio_threshold:
            results.append((face_img, "Unknown", sim1, facial_area))
            continue

        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ –∫–æ—Å–∏–Ω—É—Å–Ω–∞ –ø–æ–¥—ñ–±–Ω—ñ—Å—Ç—å –¥–æ—Å—Ç–∞—Ç–Ω—è –∑ –Ω–∞–π–±–ª–∏–∂—á–∏–º —Ü–µ–Ω—Ç—Ä–æ—ó–¥–æ–º
        thr = INFERENCE_THRESHOLDS.get(actor1, default_threshold)
        if sim1 < thr:
            results.append((face_img, "Unknown", sim1, facial_area))
            continue

        # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –∞–∫—Ç–æ—Ä–∞ —Ç–∞ –æ–±–ª–∏—á—á—è –≤ —Ä–∞–∑—ñ —É—Å–ø—ñ—Ö—É
        results.append((face_img, actor1, sim1, facial_area))

    return results

def brush_stroke_frame(image, x, y, w, h, stroke_color=(41, 128, 185), stroke_width=None, num_strokes=4):
    """
    –°—Ç–≤–æ—Ä—é—î –º—ñ–Ω—ñ–º–∞–ª—ñ—Å—Ç–∏—á–Ω—É —Ä–∞–º–∫—É –∑ –º–∞–∑–∫—ñ–≤ –ø–µ–Ω–∑–ª—è –Ω–∞–≤–∫–æ–ª–æ –æ–±–ª–∏—á—á—è.

    """

    # –î–æ–¥–∞—î–º–æ –≤—ñ–¥—Å—Ç—É–ø –¥–ª—è —Ä–∞–º–∫–∏
    padding = int(min(w, h) * 0.2)

    # –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç–∏ —Ä–∞–º–∫–∏ –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –≤—ñ–¥—Å—Ç—É–ø—É
    x1, y1 = max(0, x - padding), max(0, y - padding)
    x2, y2 = min(image.shape[1], x + w + padding), min(image.shape[0], y + h + padding)

    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Ç–æ–≤—â–∏–Ω–∏ –º–∞–∑–∫–∞
    if stroke_width is None:
        stroke_width = max(3, int(min(w, h) / 25))

    # –û–±–º–µ–∂—É—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –º–∞–∑–∫—ñ–≤ –≤—ñ–¥ 1 –¥–æ 4
    num_strokes = max(1, min(4, num_strokes))

    # –ú–∞–ª—é—î–º–æ –º–∞–∑–∫–∏ –ø–µ–Ω–∑–ª—è
    draw_brush_strokes(image, x1, y1, x2, y2, stroke_color, stroke_width, num_strokes)


def draw_brush_strokes(image, x1, y1, x2, y2, color, width, num_strokes):
    """
    –ú–∞–ª—é—î —Ö—É–¥–æ–∂–Ω—ñ –º–∞–∑–∫–∏ –ø–µ–Ω–∑–ª—è –Ω–∞–≤–∫–æ–ª–æ –≤–∫–∞–∑–∞–Ω–æ—ó –æ–±–ª–∞—Å—Ç—ñ.

    Args:
        image: –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –¥–ª—è –º–∞–ª—é–≤–∞–Ω–Ω—è
        x1, y1: –≤–µ—Ä—Ö–Ω—ñ–π –ª—ñ–≤–∏–π –∫—É—Ç –æ–±–ª–∞—Å—Ç—ñ
        x2, y2: –Ω–∏–∂–Ω—ñ–π –ø—Ä–∞–≤–∏–π –∫—É—Ç –æ–±–ª–∞—Å—Ç—ñ
        color: –∫–æ–ª—ñ—Ä –º–∞–∑–∫—ñ–≤
        width: —Ç–æ–≤—â–∏–Ω–∞ –º–∞–∑–∫—ñ–≤
        num_strokes: –∫—ñ–ª—å–∫—ñ—Å—Ç—å –º–∞–∑–∫—ñ–≤ (1-4)
    """
    # –°—Ç–æ—Ä–æ–Ω–∏ –ø—Ä—è–º–æ–∫—É—Ç–Ω–∏–∫–∞
    sides = []

    # –í–µ—Ä—Ö–Ω—è —Å—Ç–æ—Ä–æ–Ω–∞
    if num_strokes >= 1:
        sides.append([(x1, y1), (x2, y1)])

    # –ü—Ä–∞–≤–∞ —Å—Ç–æ—Ä–æ–Ω–∞
    if num_strokes >= 2:
        sides.append([(x2, y1), (x2, y2)])

    # –ù–∏–∂–Ω—è —Å—Ç–æ—Ä–æ–Ω–∞
    if num_strokes >= 3:
        sides.append([(x2, y2), (x1, y2)])

    # –õ—ñ–≤–∞ —Å—Ç–æ—Ä–æ–Ω–∞
    if num_strokes >= 4:
        sides.append([(x1, y2), (x1, y1)])

    # –ú–∞–ª—é—î–º–æ –∫–æ–∂–µ–Ω –º–∞–∑–æ–∫ –ø–µ–Ω–∑–ª—è
    for start, end in sides:
        # –°—Ç–≤–æ—Ä—é—î–º–æ —Ç–æ—á–∫–∏ –¥–ª—è –∫—Ä–∏–≤–æ—ó
        control_points = create_brush_curve(start, end)

        # –ú–∞–ª—é—î–º–æ –æ—Å–Ω–æ–≤–Ω—É –∫—Ä–∏–≤—É
        draw_smooth_curve(image, control_points, color, width)

        # –î–æ–¥–∞—î–º–æ –Ω–µ–≤–µ–ª–∏–∫—ñ –≤–∞—Ä—ñ–∞—Ü—ñ—ó –¥–ª—è –æ—Ä–≥–∞–Ω—ñ—á–Ω–æ—Å—Ç—ñ –º–∞–∑–∫–∞
        alpha = 120  # –ü—Ä–æ–∑–æ—Ä—ñ—Å—Ç—å –¥–æ–¥–∞—Ç–∫–æ–≤–∏—Ö –º–∞–∑–∫—ñ–≤
        light_color = tuple(min(255, c + 40) for c in color)  # –°–≤—ñ—Ç–ª—ñ—à–∏–π –≤—ñ–¥—Ç—ñ–Ω–æ–∫

        # –î–æ–¥–∞—î–º–æ 1-2 –¥–æ–¥–∞—Ç–∫–æ–≤—ñ —Ç–æ–Ω–∫—ñ –º–∞–∑–∫–∏ –¥–ª—è –µ—Ñ–µ–∫—Ç—É
        for _ in range(random.randint(1, 2)):
            # –ó–º—ñ—â—É—î–º–æ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ñ —Ç–æ—á–∫–∏ –¥–ª—è –≤–∞—Ä—ñ–∞—Ü—ñ—ó
            varied_points = [(p[0] + random.randint(-3, 3), p[1] + random.randint(-3, 3))
                             for p in control_points]

            # –ú–∞–ª—é—î–º–æ –¥–æ–¥–∞—Ç–∫–æ–≤–∏–π –º–∞–∑–æ–∫
            draw_smooth_curve(image, varied_points, light_color, max(1, width // 3), alpha)


def create_brush_curve(start, end):
    """
    –°—Ç–≤–æ—Ä—é—î –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ñ —Ç–æ—á–∫–∏ –¥–ª—è –º–∞–∑–∫–∞ –ø–µ–Ω–∑–ª—è –º—ñ–∂ –¥–≤–æ–º–∞ —Ç–æ—á–∫–∞–º–∏.

    Args:
        start: –ø–æ—á–∞—Ç–∫–æ–≤–∞ —Ç–æ—á–∫–∞ (x, y)
        end: –∫—ñ–Ω—Ü–µ–≤–∞ —Ç–æ—á–∫–∞ (x, y)

    Returns:
        list: —Å–ø–∏—Å–æ–∫ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω–∏—Ö —Ç–æ—á–æ–∫ –¥–ª—è –∫—Ä–∏–≤–æ—ó
    """
    # –í–∏–∑–Ω–∞—á–∞—î–º–æ –ø—Ä–æ–º—ñ–∂–Ω—ñ —Ç–æ—á–∫–∏
    length = math.sqrt((end[0] - start[0]) ** 2 + (end[1] - start[1]) ** 2)
    num_points = max(5, int(length / 20))

    # –°—Ç–≤–æ—Ä—é—î–º–æ –±–∞–∑–æ–≤—ñ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ñ —Ç–æ—á–∫–∏
    points = []
    for i in range(num_points):
        t = i / (num_points - 1)
        x = int(start[0] * (1 - t) + end[0] * t)
        y = int(start[1] * (1 - t) + end[1] * t)

        # –î–æ–¥–∞—î–º–æ —Ö—É–¥–æ–∂–Ω—é –Ω–µ–¥–æ—Å–∫–æ–Ω–∞–ª—ñ—Å—Ç—å (–≤–∞—Ä—ñ–∞—Ü—ñ—é)
        max_deviation = int(length / 15)

        # –ù–∞ –ø–æ—á–∞—Ç–∫—É —ñ –∫—ñ–Ω—Ü—ñ –º–µ–Ω—à–µ –≤—ñ–¥—Ö–∏–ª–µ–Ω–Ω—è
        factor = 4 * t * (1 - t)  # –º–∞–∫—Å–∏–º—É–º —É —Ü–µ–Ω—Ç—Ä—ñ, –º—ñ–Ω—ñ–º—É–º –Ω–∞ –∫—ñ–Ω—Ü—è—Ö
        deviation_x = int(random.randint(-max_deviation, max_deviation) * factor)
        deviation_y = int(random.randint(-max_deviation, max_deviation) * factor)

        points.append((x + deviation_x, y + deviation_y))

    # –ó–∞–±–µ–∑–ø–µ—á—É—î–º–æ, —â–æ–± –ø–æ—á–∞—Ç–∫–æ–≤–∞ —ñ –∫—ñ–Ω—Ü–µ–≤–∞ —Ç–æ—á–∫–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞–ª–∏
    points[0] = start
    points[-1] = end

    return points


def draw_smooth_curve(image, points, color, width, alpha=255):
    """
    –ú–∞–ª—é—î –ø–ª–∞–≤–Ω—É –∫—Ä–∏–≤—É –ø–æ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω–∏—Ö —Ç–æ—á–∫–∞—Ö.

    Args:
        image: –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –¥–ª—è –º–∞–ª—é–≤–∞–Ω–Ω—è
        points: —Å–ø–∏—Å–æ–∫ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω–∏—Ö —Ç–æ—á–æ–∫
        color: –∫–æ–ª—ñ—Ä –∫—Ä–∏–≤–æ—ó
        width: —Ç–æ–≤—â–∏–Ω–∞ –∫—Ä–∏–≤–æ—ó
        alpha: –ø—Ä–æ–∑–æ—Ä—ñ—Å—Ç—å (0-255)
    """
    # –°—Ç–≤–æ—Ä—é—î–º–æ —à–∞—Ä –¥–ª—è –º–∞–ª—é–≤–∞–Ω–Ω—è –∑ –ø—Ä–æ–∑–æ—Ä—ñ—Å—Ç—é
    overlay = image.copy()

    # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ —Å–ø–∏—Å–æ–∫ —Ç–æ—á–æ–∫ —É –º–∞—Å–∏–≤ —Ç–æ—á–æ–∫ –¥–ª—è OpenCV
    points_array = np.array(points, dtype=np.int32)

    # –ú–∞–ª—é—î–º–æ –ø–ª–∞–≤–Ω—É –∫—Ä–∏–≤—É —á–µ—Ä–µ–∑ —Ç–æ—á–∫–∏
    for i in range(len(points) - 1):
        pt1 = points[i]
        pt2 = points[i + 1]

        # –í–∞—Ä—ñ—é—î–º–æ —Ç–æ–≤—â–∏–Ω—É –¥–ª—è –µ—Ñ–µ–∫—Ç—É –º–∞–∑–∫–∞ –ø–µ–Ω–∑–ª—è
        local_width = max(1, int(width * (0.8 + 0.4 * random.random())))

        cv2.line(overlay, pt1, pt2, color, local_width, cv2.LINE_AA)

    # –ù–∞–∫–ª–∞–¥–∞—î–º–æ —à–∞—Ä –∑ –ø—Ä–æ–∑–æ—Ä—ñ—Å—Ç—é
    cv2.addWeighted(overlay, alpha / 255, image, 1 - alpha / 255, 0, image)


DETECTION_THRESHOLD = 0.7
INDEX, ACTOR_MAP, INFERENCE_THRESHOLDS, UKR_ACTOR_NAMES = load_recognition_resources(
    index_path=index_path,
    actors_map_path=actor_map_path,
    thresholds_path=actor_distance_thresholds_path,
    ukr_actor_names_path=ukr_actor_names_path
)


def get_face_embeddings(
        img: np.ndarray,
        default_threshold: float = 0.4,
        ratio_threshold: float = 1.3,
        model_name: str = 'Facenet512',
        detector_backend: str = 'mtcnn',
        align: bool = True,
        normalization: str = 'base'
) -> List[Tuple[np.ndarray, str, float, dict]]:
    """
    1) –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î FAISS-—ñ–Ω–¥–µ–∫—Å —Ç–∞ actor_map
    2) –î–µ—Ç–µ–∫—Ç—É—î –≤—Å—ñ –æ–±–ª–∏—á—á—è —É img —ñ —Ä–æ–±–∏—Ç—å –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –µ–º–±–µ–¥–∏–Ω–≥
    3) –®—É–∫–∞—î –¥–≤–æ—Ö –Ω–∞–π–±–ª–∏–∂—á–∏—Ö —Å—É—Å—ñ–¥—ñ–≤ (–¥–ª—è ratio-test)
    4) –î–ª—è –∫–æ–∂–Ω–æ–≥–æ –æ–±–ª–∏—á—á—è –ø–æ–≤–µ—Ä—Ç–∞—î –∫–æ—Ä—Ç–µ–∂ (face_img, actor, similarity, facial_area)
       –∞–±–æ (face_img, "Unknown", similarity, facial_area), —è–∫—â–æ –ø–æ—Ä—ñ–≥ –Ω–µ –ø—Ä–æ–π–¥–µ–Ω–∏–π.
    """
    # –ü–µ—Ä–µ–∫–æ–Ω–∞—î–º–æ—Å—å —É –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—ñ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
    img = ensure_uint8(img)

    # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –≤—Å—ñ –ª–∏—Ü—è –Ω–∞ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—ñ
    faces = detection.extract_faces(
        img_path=img,
        detector_backend=detector_backend,
        enforce_detection=False,
        align=align
    )

    h, w = img.shape[:2]

    # –í—ñ–¥—Ñ—ñ–ª—å—Ç—Ä–æ–≤—É—î–º–æ —Ö–∏–±–Ω—ñ –¥–µ—Ç–µ–∫—Ü—ñ—ó –∑–∞ —Ä–æ–∑–º—ñ—Ä–æ–º
    faces = [
        f for f in faces
            if f.get('face') is not None and f['face'].size > 0
            and f['face'].shape[0] > 0 and f['face'].shape[1] > 0
            and f['face'].shape[0] < h and f['face'].shape[1] < w
            ]

    results = []

    for face_info in faces:
        # –í–∏—Ç—è–≥—É—î–º–æ —Å–∞–º–µ –ª–∏—Ü–µ –∑ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è —Ç–∞ –π–æ–≥–æ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∏
        face_img = face_info['face']

        # –ü–µ—Ä–µ–∫–æ–Ω–∞—î–º–æ—Å—å —É –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—ñ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
        face_img = ensure_uint8(face_img)

        # –û—Ç—Ä–∏–º—É—î–º–æ –µ–º–±–µ–¥–∏–Ω–≥
        rep = representation.represent(
            img_path=face_img,
            model_name=model_name,
            enforce_detection=True,
            detector_backend='skip',
            align=align,
            normalization=normalization
        )
        q = np.array(rep[0]['embedding'], dtype=np.float32)
        q /= np.linalg.norm(q)
        q = q.reshape(1, -1)

        # –®—É–∫–∞—î–º–æ 2 –Ω–∞–π–±–ª–∏–∂—á–∏—Ö —Ü–µ–Ω—Ç—Ä–æ—ó–¥–∏
        D, I = INDEX.search(q, 2)
        sim1, sim2 = float(D[0][0]), float(D[0][1])
        actor1 = ACTOR_MAP[int(I[0][0])]

        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ –µ–º–±–µ–¥–∏–Ω–≥ –∑–Ω–∞—Ö–æ–¥–∏—Ç—å—Å—è –±–ª–∏–∑—å–∫–æ –¥–æ –∫—ñ–ª—å–∫–æ—Ö —Ü–µ–Ω—Ç—Ä–æ—ó–¥—ñ–≤ –æ–¥—Ä–∞–∑—É
        if sim1 / (sim2 + 1e-8) < ratio_threshold:
            results.append(("Unknown", q))
            continue

        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ –∫–æ—Å–∏–Ω—É—Å–Ω–∞ –ø–æ–¥—ñ–±–Ω—ñ—Å—Ç—å –¥–æ—Å—Ç–∞—Ç–Ω—è –∑ –Ω–∞–π–±–ª–∏–∂—á–∏–º —Ü–µ–Ω—Ç—Ä–æ—ó–¥–æ–º
        thr = INFERENCE_THRESHOLDS.get(actor1, default_threshold)
        if sim1 < thr:
            results.append(("Unknown", q))
            continue

        # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –∞–∫—Ç–æ—Ä–∞ —Ç–∞ –æ–±–ª–∏—á—á—è –≤ —Ä–∞–∑—ñ —É—Å–ø—ñ—Ö—É
        results.append((actor1, q))

    return results

def process_video_embeddings(
        video_path: str,
        frame_step_sec: float = 1.0
) -> List[Tuple[np.ndarray, str, float]]:
    """
    –û–±—Ä–æ–±–ª—è—î –≤—ñ–¥–µ–æ –ø–æ –∫–∞–¥—Ä–∞—Ö, –æ—Ç—Ä–∏–º—É—é—á–∏ –µ–º–±–µ–¥–∏–Ω–≥–∏ –æ–±–ª–∏—á —ñ–∑ —á–∞—Å–æ–≤–∏–º–∏ –º—ñ—Ç–∫–∞–º–∏.

    Parameters:
    - video_path: —à–ª—è—Ö –¥–æ –≤—ñ–¥–µ–æ (—Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª –ø—ñ—Å–ª—è upload)
    - recognize_fn: —Ñ—É–Ω–∫—Ü—ñ—è —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è, —è–∫–∞ –ø–æ–≤–µ—Ä—Ç–∞—î: (img, name, sim, box, emb)
    - frame_step_sec: –∫—Ä–æ–∫ —É —Å–µ–∫—É–Ω–¥–∞—Ö –º—ñ–∂ –∫–∞–¥—Ä–∞–º–∏

    Returns:
    - –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂—ñ–≤ (embedding, actor_name, timestamp)
    """

    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        st.error("–ù–µ –≤–¥–∞–ª–æ—Å—è –≤—ñ–¥–∫—Ä–∏—Ç–∏ –≤—ñ–¥–µ–æ—Ñ–∞–π–ª.")
        return []

    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    frame_step = int(fps * frame_step_sec)
    embeddings_data = []

    # st.info("üîÑ –ó–Ω–∞—Ö–æ–¥–∏–º–æ —Ç–∞ —Ä–æ–∑–ø—ñ–∑–Ω–∞—î–º–æ –æ–±–ª–∏—á—á—è")
    progress = st.progress(0)

    for frame_idx in range(0, total_frames, frame_step):
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
        success, frame = cap.read()
        if not success:
            continue

        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        rgb_frame = ensure_uint8(rgb_frame)

        try:
            results = get_face_embeddings(rgb_frame)  # (img, name, sim, box, emb)
            timestamp = frame_idx / fps

            for actor_name, emb in results:
                embeddings_data.append((emb, actor_name, timestamp))

        except Exception as e:
            st.warning(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –Ω–∞ –∫–∞–¥—Ä—ñ {frame_idx}: {str(e)}")
            continue

        # –û–Ω–æ–≤–ª–µ–Ω–Ω—è –ø—Ä–æ–≥—Ä–µ—Å-–±–∞—Ä—É
        percent = frame_idx / total_frames
        progress.progress(min(int(percent * 100), 100))

    cap.release()
    progress.progress(100)
    return embeddings_data

def cosine_sim(a, b):
    a = a.ravel()
    b = b.ravel()
    return np.clip(np.dot(a, b), -1.0, 1.0)


def track_actors_online(frames_embeddings: List[Tuple[np.ndarray, str, float]]) -> List[dict]:
    """
    –û–Ω–ª–∞–π–Ω-–∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—è –µ–º–±–µ–¥–∏–Ω–≥—ñ–≤ –Ω–∞ –æ—Å–Ω–æ–≤—ñ cosine similarity
    frames_embeddings ‚Äî —Å–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂—ñ–≤: (embedding, actor_name, timestamp)
    """
    known_clusters = []  # —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–Ω–∏–∫—ñ–≤: {id, embeddings, actor_names, timestamps}
    threshold = 0.75  # —è–∫—â–æ cosine similarity > threshold ‚Äî –≤–≤–∞–∂–∞—î—Ç—å—Å—è —Ç—ñ—î—é –∂ –æ—Å–æ–±–æ—é
    next_cluster_id = 0

    for embedding, actor_name, timestamp in frames_embeddings:

        matched = False
        for cluster in known_clusters:
            similarities = [cosine_sim(embedding, e) for e in cluster['embeddings']]
            if max(similarities) > threshold:
                cluster['embeddings'].append(embedding)
                cluster['actor_names'].append(actor_name)
                cluster['timestamps'].append(timestamp)
                matched = True
                break

        if not matched:
            known_clusters.append({
                'id': next_cluster_id,
                'embeddings': [embedding],
                'actor_names': [actor_name],
                'timestamps': [timestamp]
            })
            next_cluster_id += 1

    # –§–æ—Ä–º—É—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    results = []
    for cluster in known_clusters:
        names = [n for n in cluster['actor_names'] if n != 'Unknown']
        identity = (
            max(set(names), key=names.count) if names else f"Unknown #{cluster['id']}"
        )

        results.append({
            'identity': identity.replace("_", " "),
            'count': len(cluster['timestamps']),
            'timestamps': cluster['timestamps']
        })

    return results

def localize_identities(stats: List[dict], name_map: dict) -> List[dict]:
    """
    –ó–∞–º—ñ–Ω–∞ –∞–Ω–≥–ª—ñ–π—Å—å–∫–∏—Ö —ñ–º–µ–Ω –Ω–∞ —É–∫—Ä–∞—ó–Ω—Å—å–∫—ñ, —ñ –ø–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è Unknown ‚Üí –ù–µ–≤—ñ–¥–æ–º–∞ –æ—Å–æ–±–∞ #N (—ñ–∑ –ø–µ—Ä–µ–Ω—É–º–µ—Ä–∞—Ü—ñ—î—é).
    """
    localized = []
    unknown_counter = 1

    for entry in stats:
        identity = entry["identity"]

        # –Ø–∫—â–æ —î –ø–µ—Ä–µ–∫–ª–∞–¥ —É —Å–ª–æ–≤–Ω–∏–∫—É
        if identity in name_map:
            localized_name = name_map[identity]
        elif identity.lower().startswith("unknown"):
            localized_name = f"–ù–µ–≤—ñ–¥–æ–º–∞ –æ—Å–æ–±–∞ #{unknown_counter}"
            unknown_counter += 1
        else:
            localized_name = identity  # –∑–∞–ª–∏—à–∞—î–º–æ —è–∫ —î

        localized.append({
            "identity": localized_name,
            "count": entry["count"],
            "timestamps": entry.get("timestamps", [])
        })

    return localized


def merge_duplicate_identities(stats: List[dict]) -> List[dict]:
    """
    –û–±‚Äô—î–¥–Ω—É—î –∫–ª–∞—Å—Ç–µ—Ä–∏ –∑ –æ–¥–Ω–∞–∫–æ–≤–∏–º —ñ–º–µ–Ω–µ–º, —Å—É–º—É—é—á–∏ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø–æ—è–≤.
    """
    merged = defaultdict(lambda: {"count": 0, "timestamps": []})
    for entry in stats:
        identity = entry["identity"]
        merged[identity]["count"] += entry["count"]
        merged[identity]["timestamps"].extend(entry.get("timestamps", []))

    return [
        {"identity": identity, "count": data["count"], "timestamps": data["timestamps"]}
        for identity, data in merged.items()
    ]

def sort_by_count_then_identity(entry):
    count = entry['count']
    is_unknown = entry['identity'].startswith("–ù–µ–≤—ñ–¥–æ–º–∞ –æ—Å–æ–±–∞")
    return (-count, is_unknown)  # —Å–ø–æ—á–∞—Ç–∫—É –∑–∞ count ‚Üì, –ø–æ—Ç—ñ–º –Ω–µ–≤—ñ–¥–æ–º—ñ ‚Äî –≤–Ω–∏–∑—É


def generate_statistics_once():
    if not st.session_state.stats_ready:
        with st.spinner("üïµÔ∏è‚Äç‚ôÇÔ∏è –ó–Ω–∞—Ö–æ–¥–∏–º–æ —Ç–∞ —Ä–æ–∑–ø—ñ–∑–Ω–∞—î–º–æ –æ–±–ª–∏—á—á—è..."):
            embeddings_data = process_video_embeddings(
                video_path=st.session_state.temp_file_path,
                frame_step_sec=1.0
            )
            if not embeddings_data:
                st.warning("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –≤–∏—Ç—è–≥—Ç–∏ –µ–º–±–µ–¥–∏–Ω–≥–∏ –∑ –≤—ñ–¥–µ–æ.")
                return

            final_stats = track_actors_online(embeddings_data)
            if not final_stats:
                st.warning("üòê –û–±–ª–∏—á—á—è –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∞–±–æ –Ω–µ –≤–¥–∞–ª–æ—Å—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑—É–≤–∞—Ç–∏.")
                return

            merged_stats_en = merge_duplicate_identities(final_stats)
            merged_stats = localize_identities(merged_stats_en, UKR_ACTOR_NAMES)
            merged_stats.sort(key=sort_by_count_then_identity)

            st.session_state.stats_ready = True
            st.session_state.final_stats = merged_stats

def reset_state():
    st.session_state.video_file = None
    st.session_state.temp_file_path = None
    st.session_state.cap = None
    st.session_state.frame = None
    st.session_state.frame_image = None
    st.session_state.recognition_results = None
    st.session_state.detector_loaded = False
    st.session_state.current_timestamp = 0.0
    st.session_state.slider_key = 0
    st.session_state.stats_ready = False
    st.session_state.final_stats = None
